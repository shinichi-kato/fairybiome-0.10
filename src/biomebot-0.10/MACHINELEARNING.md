machine learning
===============================

LLMは巨大な学習データとAttentionに注目した学習モデルで今までにない対話能力を
獲得した。チャットボットのバックエンドにLLMを利用することは急速にコモディティ化した
技術となっているが、LLMは巨大故に個人の計算資源で賄うのは困難である。また
プロンプトエンジニアリングによって一定の範囲で反応の傾向を制御することやユーザとの
やり取りを学習させることは可能であるが、現状では一貫性や精度を保つことが難しい。

一方で従来型のチャットボットはLLMのスケールと比べて非常に小さい学習データを
用い、会話能力はあまり高くない。
一方でチャットボットの製作者は雑談をさせるのに暗黙的な戦略を検討し、それを
学習データの形でチャットボットに与える。

その情報をインターネット上に漏洩しない対策が必要になる。さらにチャットボットの
製作者は雑談をさせるのに暗黙的に様々な戦略を用い、それを学習データの形で
与えているが、LLMのスケールから比べて非常に小さい学習データしか用意することは
できない。

※ ブラウザに乗せる意義は？
※ 
これらのことから

javascript上の小さい計算資源で小さい辞書を用い、できるだけ広い範囲の
テキストマッチングを行う

## トークン化
機械学習の手法によらず、テキストは1hot-vectorに変換される。学習データが
巨大になるほどスパースになり効率や精度に悪影響を及ぼすため、トークン化に
よってベクトルの次元を一定に制限する。英語の場合はスペース区切りが最も
単純な例であるが、materializeをmaterial＋izeに分割するなどの細分化を取り入れる
工夫がされている。日本語の場合は形態素解析がよく行われる

英語における従来の類似テキスト検出では、スペース区切りされた単語を最小単位と
位置づけ、単語を次元とした1-hotベクターを内部表現として自然言語処理を行った。
日本語の場合は形態素やそれをベースにした分かち書きの結果を最小単位としており、
英単語と比べてより細かい分割が行われている。上述の最小単位を以下ノードと呼ぶ。

チャットボットで返答を生成するには、入力文字列と出力文字列のペアからなる
辞書を用意し、ユーザの発言が辞書の入力文字列と類似していればそれに対応した
出力文字列を返答とする、というのが最もかんたんな方法の一つで、テキストを

「ノードを次元としたベクター」に変換し、ベクター同士のcos類似度を計算すれば
類似したテキストを見つけることができる。

この方法では単語ベースで似たテキストは検出できても違う言葉で意味が似ている
表現は検出できない。目標の単語の周囲の単語の並びを利用するn-gramはこれを
改善できる可能性があり、n-gramを使ったword2vecでは似た単語はベクトルも
似ていることが期待される。そこで、辞書を一つのコーパスと見立ててコーパス内の
類似単語を同じとみなすような処理をすれば、「単語が違っていても似た表現」に
対応できると考えられる。
ここで、日本語の場合、ノードが形態素レベルに分割されていることが精度を
下げる原因となる。例えば「日本語」は「日本/語」と分割される。その場合

1. 複合語を一つのノードに統合する
2. コーパスをword2vecでベクトル化し、k-meansなどのクラスタリング手法で
類似単語をクラスタ化する
3. クラスタに属する語を同じ意味とみなすためタグに置き換える
4. タグに置き換えたテキストでベクターを生成し、cos類似度で類似テキストを
特定する


### 複合語辞書 compounds.json
[
    [代表surface,surfaces...],
    ...
]

### クラスタ辞書 clusters.json
[
    [代表surface, surfaces...]
]




## 学習モデル生成

## 

